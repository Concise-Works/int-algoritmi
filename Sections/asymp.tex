

\section{Asymptotic Notation}

Asymptotic analysis is a method for describing the limiting behavior of functions as inputs grow infinitely.
\underline{\textbf{Note:} $1 < \log n < n < n \log n < n^2 < n^3 < 2^n < n!$.}

\begin{Def}[Asymptotic]
    
    Let \(f(n)\) and \(g(n)\) be two functions. As \(n\) grows, if \(f(n)\) grows closer to \(g(n)\) never reaching, we say that \underline{``$f(n)$ is \textbf{asymptotic} to \(g(n)\).''}\\
    
    \noindent
    We call the point where \(f(n)\) starts behaving similarly to \(g(n)\) the \underline{\textbf{threshold} \(n_0\).} After this point $n_0$, \(f(n)\) follows the same general path as \(g(n)\).
\end{Def}

\begin{Def}[Big-O: (Upper Bound)]

    \label{def:bigO}

    Let $f$ and $g$ be functions. $f(n)$ our function of interest, and $g(n)$
    our function of comparison.\\

    \noindent
    Then we say $f(n)=O(g(n))$, ``$f(n)$ \textbf{is big-O of} $g(n)$,'' if $f(n)$ 
    grows no faster than $g(n)$, up to a constant factor.
    Let $n_0$ be our asymptotic threshold. Then, for all $n\geq n_0$,
    \large
    \[0\leq f(n) \leq c\cdot g(n)\]
    \normalsize
    
    \noindent
    Represented as the ratio $\dfrac{f(n)}{g(n)}\leq c$ for all $n\geq n_0$. Analytically we write,
    \Large
    \[\lim_{n\to\infty}\dfrac{f(n)}{g(n)}< \infty\]
    \normalsize
    \noindent
    Meaning, as we chase infinity, our numerator grows slower than the denominator, bounded, never reaching infinity. 
\end{Def}

\newpage

\noindent
\textbf{Examples:}
\begin{enumerate}
    \item[(i.)] $3n^2+2n+1=O(n^2)$
    \item[(ii.)] $n^{100}=O(2^n)$
    \item[(iii.)] $\log n=O(\sqrt{n})$ 
\end{enumerate}

\begin{Proof}[$\log n = O(\sqrt{n})$]

    \label{proof:logn}
We setup our ratio:
\[\lim_{n\to\infty}\dfrac{\log n}{\sqrt{n}}\]
\noindent
Since $\log n$ and $\sqrt{n}$ grow infinitely without bound, they are of indeterminate form $\frac{\infty}{\infty}$. We apply L'Hopital's Rule, which states
that taking derivatives of the numerator and denominator will yield an evaluateable limit:
\Large
\[\lim_{n\to\infty}\dfrac{\log n}{\sqrt{n}}=\lim_{n\to\infty}\dfrac{\frac{d}{dn}\log n}{\frac{d}{dn}\sqrt{n}}\]
\normalsize
\noindent
Yielding derivatives, $\log n = \frac{1}{n}$ and $\sqrt{n}=\frac{1}{2\sqrt{n}}$. We substitute these back into our limit:
\Large
\[\lim_{n\to\infty}\dfrac{\frac{1}{n}}{\frac{1}{2\sqrt{n}}}=\lim_{n\to\infty}\dfrac{2\sqrt{n}}{n}=\lim_{n\to\infty}\dfrac{2}{\sqrt{n}}=0\]
\normalsize
\noindent
Our limit approaches 0, as we have a constant factor in the numerator, and a growing denominator. Thus, $\log n = O(\sqrt{n})$, as $0<\infty$.
\end{Proof}

\begin{Def}[Big-$\Omega$: (Lower Bound)]

    \label{def:bigOmega}

    The symbol $\Omega$ reads ``Omega.'' Let $f$ and $g$ be functions. Then 
    $f(n)=\Omega(g(n))$ if $f(n)$ grows no slower than $g(n)$, up to a constant factor. I.e.,
    lower bounded by $g(n)$. Let $n_0$ be our asymptotic threshold. Then, for all $n\geq n_0$,
    \large
    \[0\leq c\cdot g(n) \leq f(n)\]
    \[0<\lim_{n\to\infty}\dfrac{f(n)}{g(n)}\]
    \normalsize
    \noindent
    Meaning, as we chase infinity, our numerator grows faster than the denominator, approaching 0 asymptotically.
\end{Def}

\noindent
\textbf{Examples:} $n!=\Omega(2^n)$; $\dfrac{n}{100}= \Omega(n)$; $n^{3/2}= \Omega(\sqrt{n})$; $\sqrt{n} = \Omega(\log n)$

\newpage

\begin{Def}[Big $\Theta$: (Tight Bound)]

    \label{def:bigTheta}
    The symbol $\Theta$ reads ``Theta.'' Let $f$ and $g$ be functions. Then $f(n)=\Theta(g(n))$ if $f(n)$ grows at the same rate as $g(n)$, up to a constant factor. I.e., $f(n)$ is both upper and lower bounded by $g(n)$. Let $n_0$ be our asymptotic threshold, and $c_1>0,c_2>0$ be some constants. Then, for all $n\geq n_0$,
    \large
    \[0\leq c_1\cdot g(n) \leq f(n) \leq c_2\cdot g(n)\]
    \[0<\lim_{n\to\infty}\dfrac{f(n)}{g(n)}<\infty\]
    \normalsize
    \noindent
    Meaning, as we chase infinity, our numerator grows at the same rate as the denominator.
\end{Def}
\noindent
\textbf{Examples:} $n^2=\Theta(n^2)$; $2n^3+2n=\Theta(n^3)$; $\log n+\sqrt{n}=\Theta(\sqrt{n})$.
\begin{Def}[Little $o$: (Strict Upper Bound)]

    The symbol $o$ reads ``little-o.'' Let $f$ and $g$ be functions. Then $f(n)=o(g(n))$ if $f(n)$ grows strictly slower than $g(n)$, meaning $f(n)$ becomes insignificant compared to $g(n)$ as $n$ grows large. Let $n_0$ be our asymptotic threshold. Then, for all $n\geq n_0$,
    \large
    \[0\leq f(n) < c \cdot g(n)\]
    \[\lim_{n\to\infty}\dfrac{f(n)}{g(n)}=0\]
    \normalsize
    \noindent
    Meaning, as we chase infinity, the ratio of $f(n)$ to $g(n)$ shrinks to zero.
\end{Def}
\noindent
\textbf{Examples:} $n=\!o(n^2)$; $\log n=\!o(n)$; $n^{0.5}=\!o(n)$.
\begin{Def}[Little $\omega$: (Strict Lower Bound)]

    The symbol $\omega$ reads ``little-omega.'' Let $f$ and $g$ be functions. Then $f(n)=\omega(g(n))$ if $f(n)$ grows strictly faster than $g(n)$, meaning $g(n)$ becomes insignificant compared to $f(n)$ as $n$ grows large. Let $n_0$ be our asymptotic threshold. Then, for all $n\geq n_0$,
    \large
    \[0\leq c \cdot g(n) < f(n)\]
    \[\lim_{n\to\infty}\dfrac{g(n)}{f(n)}=0\]
    \normalsize
    \noindent
    Meaning, as we chase infinity, the ratio of $g(n)$ to $f(n)$ shrinks to zero.
\end{Def}
\noindent
\textbf{Examples:} $n^2=\omega(n)$; $n=\omega(\log n)$.

\newpage 

\begin{Def}[Asymptotic Equality (\( \sim \))]

    The symbol \( \sim \) reads ``asymptotic equality.'' Let $f$ and $g$ be functions. Then $f(n) \sim g(n)$ if, as $n \to \infty$, the ratio of $f(n)$ to $g(n)$ approaches 1. I.e., the two functions grow at the same rate asymptotically. Formally,
    \large
    \[\lim_{n\to\infty}\dfrac{f(n)}{g(n)}=1\]
    \normalsize
    \noindent
    Meaning, as $n$ grows large, the two functions become approximately equal.
\end{Def}
\noindent
\textbf{Examples:} $n + 100 \sim n, \quad \log(n^2) \sim 2\log n.$


\begin{Tip}
    To review:
    \begin{itemize}
        \item \textbf{Big-$O$:} $f(n)$ < $g(n)$ (Upper Bound); $f(n)$ grows no faster than $g(n)$.
        \item \textbf{Big-$\Omega$:} $f(n)$ > $g(n)$ (Lower Bound); $f(n)$ grows no slower than $g(n)$.
        \item \textbf{Big-$\Theta$:} $f(n)$ = $g(n)$ (Tight Bound); $f(n)$ grows at the same rate as $g(n)$.
        \item \textbf{Little-$o$:} $f(n)$ < $g(n)$ (Strict Upper Bound); $f(n)$ grows strictly slower than $g(n)$.
        \item \textbf{Little-$\omega$:} $f(n)$ > $g(n)$ (Strict Lower Bound); $f(n)$ grows strictly faster than $g(n)$.
        \item \textbf{Asymptotic Equality:} $f(n)$ $\sim$ $g(n)$; $f(n)$ grows at the same rate as $g(n)$.
    \end{itemize}
\end{Tip}

\begin{theo}[Types of Asymptotic Behavior]

    The following are common relationships between different types of functions and their asymptotic growth rates:

    \begin{itemize}
        \item \textbf{Polynomials.} Let $f(n) = a_0 + a_1 n + \dots + a_d n^d$ with $a_d > 0$. Then, \underline{$f(n)$ is $\Theta(n^d)$.}\\
        E.e., $3n^2+2n+1$ is $\Theta(n^2)$.
        
        \item \textbf{Logarithms.} \underline{$\Theta(\log_a n)$ is $\Theta(\log_b n)$} for any constants $a, b > 0$. That is, logarithmic functions in different bases have the same growth rate.\\
        E.g., $\log_2 n$ is $\Theta(\log_3 n)$.
        
        \item \textbf{Logarithms and Polynomials.} For every $d > 0$, \underline{$\log n$ is $O(n^d)$.} This indicates that logarithms grow slower than any polynomial.\\
        E.g., $\log n$ is $O(n^2)$.
        
        \item \textbf{Exponentials and Polynomials.} For every $r > 1$ and every $d > 0$, \underline{$n^d$ is $O(r^n)$.} This means that exponentials grow faster than any polynomial.\\
        E.e., $n^2$ is $O(2^n)$.
    \end{itemize}
\end{theo}

\newpage 

\begin{Proof}[$O(logn)=O(n)$]

    \label{proof:logn}
    We prove that $\log_2 n$ is $O(n)$ by induction. Claim: for all $n\geq 1$, $\log_2 n \leq n$.\\
      \textbf{Inductive step:} Assume $n = k$ for some $k\geq 1$. We show that $n = k+1$ holds.

        \vspace{-1em}
        \begin{align*}
            \log_2(k+1) \leq \log_2(2k) &\quad \text{(Choosing $2k$ as a convenient upper bound.)}\\
            \log_2(2k) = \log_2 k + \log_2 2 &\quad \text{(Product Rule)}\\
            \log_2(2k) = \log_2 k + 1 &\quad \text{(Simplifying.)}\\
            \log_2(k+1) \leq \log_2 k + 1 &\quad \text{(Substituting.)}
        \end{align*}
        
        \noindent
        Hence, $\log_2(k+1) \leq k+1$. Thus, by induction, $\log_2 n$ is $O(n)$.
\end{Proof}

\begin{Def}[Time Complexity \& Space Complexity]
    
        In terms of input size, The \textbf{time complexity} measures the run-time of an algorithm.
        \textbf{Space complexity} measures the memory usage of an algorithm. Both are expressed in asymptotic notation.
\end{Def}

\begin{Func}[Arithmetic Series - \texttt{Fun1($A$)}]
    Computes a result based on a length-$n$ array of integers:

    \vspace{.5em}
    \noindent
    \textbf{Input: } A length-$n$ array of integers.\\
    \textbf{Output: } An integer $p$ computed from the array elements.\\
    \begin{algorithm}[H]
        \SetAlgoLined
        
        \vspace{.5em}
        \SetKwProg{Fn}{Function}{:}{\KwRet{$p$}}
        \Fn{\texttt{Fun1($A$)}}{
            $p \gets 0$\;
            \For{$i \gets 1$ \KwTo $n-1$}{
                \For{$j \gets i+1$ \KwTo $n$}{
                    $p \gets p + A[i] \cdot A[j]$\;
                }
            }
        }
    \end{algorithm}

    \noindent
    \textbf{Time Complexity:} For $f(n):=$ \texttt{Fun1($A$)}, $f(n)=\frac{n^2}{2}=O(n^2)$. This is because the function has a nested loop structure, where the inner for-loop runs $n-i$ times, and the outer for-loop runs $n-1$ times. Thus, the total number of iterations is $\sum_{i=1}^{n-1}n-i=\frac{n^2}{2}$.\\

    \noindent
    \textbf{Space Complexity:} We yield $O(n)$ for storing an array of length $n$. The variable $p$ is $O(1)$ (constant), as it is a single integer. Hence, $f(n)=n+1=O(n)$.
    \end{Func}   

    \noindent
    \textbf{Additional Example:} Let $f(n,m) = n^2m + m^3 + nm^3$. Then, $f(n,m)=O(n^2m+m^3)$. This is because both $n$ and $m$ must be accounted for. Our largest $n$ term is $n^2m$, and our largest $m$ term is $m^3$ both dominate the expression. Thus, $f(n,m)=O(n^2m+m^3)$.



